% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iRF.R
\name{iRF}
\alias{iRF}
\title{iterative Random Forest}
\usage{
iRF(
  x,
  y,
  iter = 5,
  mtry = NULL,
  classification = FALSE,
  saveall = TRUE,
  usepvals = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{x}{Matrix. Features as columns (i.e. the predictors for random forest), samples as rows}

\item{y}{Numeric. The dependent variable (i.e. the variable being predicted)}

\item{iter}{Integer. number of iterations to run. Default = 5.}

\item{mtry}{Numeric. Number of variables to possibly split at in each node.
Default is the (rounded down) square root of the number variables with importance > 0.
If 0 < mtry < 1, then it will be treated as a proportion of variables with importance > 0.
Otherwise it is the absolute number of variables to use when splitting a node.}

\item{classification}{Boolean. If FALSE, then use Regression RF. Default = FALSE}

\item{saveall}{Boolean. if TRUE then return the final random forest from each iteration as a list of forests}

\item{usepvals}{Boolean. Apply permutation testing to feature importances in order to cull features based on P-values}

\item{verbose}{Boolean. default = TRUE}

\item{...}{further parameters for Ranger::ranger function}
}
\value{
One of two possible results:
if saveall=TRUE, returns a list of random forest objects (one per iteration).
if saveall=FALSE, returns the random forest object from the iteration that had the best model fit.
}
\description{
an algorithmic advancement of Random Forest (RF),
which takes advantage of Random Forests ability to determine feature importance,
and produces a more accurate model by iteratively creating weighted forests.
}
\details{
Iterative Random Forest (iRF) is an algorithmic advancement of Random Forest (RF),
    which takes advantage of Random Forests ability to determine feature importance, and produces a more accurate
    model by iteratively creating weighted forests. In each iteration, the importance scores from the
    previous iteration are used to weight features for the current forest. The effect is akin to a lasso regression since
    some features have their importance boosted while others go to zero and are culled from the forest.
    The result is often a more accurate set of high ranked predictors from a better fit and simpler model.

    If you set **usepvals=TRUE** then Ranger will use Altmann's method for calculating importance P-values using permutation.
    iRF will then cull features based on having a conservative False Discovery Rate Q > 0.2 (i.e. it sets their importance to 0).
    This usually results in a harsher culling of features than just allowing importances to naturally fall to zero.
}
\examples{

data(expdata)
## use the first 99 genes' expression to predict the expression of the 100th gene
irf <- iRF(x = expdata[, 1:99], y = expdata[, 100], saveall=FALSE, num.trees = 100)
## view top 10 most important genes
## Not run:
sort(irf$variable.importance, decreasing = TRUE)[1:10]

}
\references{
Basu S, Kumbier K, Brown JB et al. (2018) Iterative random forests to discover predictive and stable high-order interactions. Proc Natl Acad Sci USA 115, 1943â€“1948.
}
\seealso{
\link[ranger]{ranger} \link[iRF]{iRF_LOOP}
}
